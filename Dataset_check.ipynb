{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json\n",
    "\n",
    "from utils import *\n",
    "from kitti_utils import *\n",
    "from layers import *\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = {\"kitti\": datasets.KITTIRAWDataset,\n",
    "                 \"kitti_odom\": datasets.KITTIOdomDataset}\n",
    "dataset = datasets_dict[\"kitti\"]\n",
    "fpath = os.path.join(\"splits\", \"eigen_zhou\", \"{}_files.txt\")\n",
    "train_filenames = readlines(fpath.format(\"train\"))\n",
    "val_filenames = readlines(fpath.format(\"val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torchvision\\transforms\\transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset(\n",
    "    \"kitti_data\", train_filenames, 192, 640,\n",
    "    [0, -1, 1], 4, is_train=True, img_ext='.png')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, 4, True,\n",
    "    num_workers=4, pin_memory=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: ('K', 0)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('inv_K', 0)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('K', 1)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('inv_K', 1)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('K', 2)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('inv_K', 2)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('K', 3)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('inv_K', 3)\n",
      "value.shape:  torch.Size([4, 4, 4])\n",
      "key: ('color', 0, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color', 0, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color', 0, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color', 0, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: ('color', -1, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color', -1, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color', -1, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color', -1, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: ('color', 1, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color', 1, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color', 1, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color', 1, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: ('color_aug', 0, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color_aug', 0, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color_aug', 0, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color_aug', 0, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: ('color_aug', -1, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color_aug', -1, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color_aug', -1, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color_aug', -1, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: ('color_aug', 1, 0)\n",
      "value.shape:  torch.Size([4, 3, 192, 640])\n",
      "key: ('color_aug', 1, 1)\n",
      "value.shape:  torch.Size([4, 3, 96, 320])\n",
      "key: ('color_aug', 1, 2)\n",
      "value.shape:  torch.Size([4, 3, 48, 160])\n",
      "key: ('color_aug', 1, 3)\n",
      "value.shape:  torch.Size([4, 3, 24, 80])\n",
      "key: depth_gt\n",
      "value.shape:  torch.Size([4, 1, 375, 1242])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for key, ipt in batch.items():\n",
    "        print(\"key:\", key)\n",
    "        print(\"value.shape: \", ipt.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 192, 640])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[(\"color_aug\", 0, 0)].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from swin_transformer_v2 import SwinTransformerV2\n",
    "\n",
    "from swin_transformer_v2 import swin_transformer_v2_t, swin_transformer_v2_s, swin_transformer_v2_b, \\\n",
    "    swin_transformer_v2_l, swin_transformer_v2_h, swin_transformer_v2_g\n",
    "\n",
    "# SwinV2-T\n",
    "swin_transformer: SwinTransformerV2 = swin_transformer_v2_t(in_channels=3,\n",
    "                                                            window_size=8,\n",
    "                                                            input_resolution=(256, 256),\n",
    "                                                            sequential_self_attention=False,\n",
    "                                                            use_checkpoint=False)\n",
    "\n",
    "swin_transformer.update_resolution(new_window_size=2, new_input_resolution=(192, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768, 6, 20])\n"
     ]
    }
   ],
   "source": [
    "# 32로 나눈 다음에 window size로 또 나눌 수 있어야함 \n",
    "for batch in train_loader:\n",
    "    depth_data = batch[(\"color_aug\", 0, 0)]\n",
    "    output=swin_transformer(depth_data)[3]\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 63, 32, 96, 2, 2]' is invalid for input of size 3096576",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mh:\\내 드라이브\\CV-paper\\Network\\Monodepth2\\Dataset_check.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dummy\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrandn(\u001b[39m4\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m510\u001b[39m,\u001b[39m256\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result\u001b[39m=\u001b[39mswin_transformer(dummy)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model.py:107\u001b[0m, in \u001b[0;36mSwinTransformerV2.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m# Forward pass of each stage\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages:\n\u001b[1;32m--> 107\u001b[0m     output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m stage(output)\n\u001b[0;32m    108\u001b[0m     features\u001b[39m.\u001b[39mappend(output)\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:761\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    759\u001b[0m         output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(block, output)\n\u001b[0;32m    760\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m         output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m block(output)\n\u001b[0;32m    762\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:451\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    449\u001b[0m output_attention: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_attention(output_patches, mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_mask)\n\u001b[0;32m    450\u001b[0m \u001b[39m# Merge patches\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m output_merge: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m fold(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49moutput_attention, window_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_size, height\u001b[39m=\u001b[39;49mheight,\n\u001b[0;32m    452\u001b[0m                                   width\u001b[39m=\u001b[39;49mwidth) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_windows \u001b[39melse\u001b[39;00m output_attention\n\u001b[0;32m    453\u001b[0m \u001b[39m# Reverse shift if utilized\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshift_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:92\u001b[0m, in \u001b[0;36mfold\u001b[1;34m(input, window_size, height, width)\u001b[0m\n\u001b[0;32m     90\u001b[0m batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m (height \u001b[39m*\u001b[39m width \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m window_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m window_size))\n\u001b[0;32m     91\u001b[0m \u001b[39m# Reshape input to\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mview(batch_size, height \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m window_size, width \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m window_size, channels,\n\u001b[0;32m     93\u001b[0m                                   window_size, window_size)\n\u001b[0;32m     94\u001b[0m output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mreshape(batch_size, channels, height, width)\n\u001b[0;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 63, 32, 96, 2, 2]' is invalid for input of size 3096576"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dummy=torch.randn(4,3,510,256)\n",
    "result=swin_transformer(dummy)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)\n",
    "print(result[2].shape)\n",
    "print(result[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 96, 128, 128])\n",
      "torch.Size([4, 192, 64, 64])\n",
      "torch.Size([4, 384, 32, 32])\n",
      "torch.Size([4, 768, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "swin_transformer.update_resolution(new_window_size=8, new_input_resolution=(512, 512))\n",
    "\n",
    "dummy=torch.randn(4,3,512,512)\n",
    "result=swin_transformer(dummy)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)\n",
    "print(result[2].shape)\n",
    "print(result[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 96, 80, 80])\n",
      "torch.Size([4, 192, 40, 40])\n",
      "torch.Size([4, 384, 20, 20])\n",
      "torch.Size([4, 768, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "swin_transformer.update_resolution(new_window_size=2, new_input_resolution=(320, 320))\n",
    "\n",
    "dummy=torch.randn(4,3,320,320)\n",
    "result=swin_transformer(dummy)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)\n",
    "print(result[2].shape)\n",
    "print(result[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 96, 48, 160])\n",
      "torch.Size([4, 192, 24, 80])\n",
      "torch.Size([4, 384, 12, 40])\n",
      "torch.Size([4, 768, 6, 20])\n"
     ]
    }
   ],
   "source": [
    "swin_transformer.update_resolution(new_window_size=2, new_input_resolution=(192, 640))\n",
    "\n",
    "dummy=torch.randn(4,3, 192, 640)\n",
    "result=swin_transformer(dummy)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)\n",
    "print(result[2].shape)\n",
    "print(result[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mh:\\내 드라이브\\CV-paper\\Network\\Monodepth2\\Dataset_check.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dummy\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrandn(\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m logits\u001b[39m=\u001b[39mswin_transformer(dummy)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(logits[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(logits[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model.py:107\u001b[0m, in \u001b[0;36mSwinTransformerV2.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39m# Forward pass of each stage\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m stage \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages:\n\u001b[1;32m--> 107\u001b[0m     output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m stage(output)\n\u001b[0;32m    108\u001b[0m     features\u001b[39m.\u001b[39mappend(output)\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:761\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    759\u001b[0m         output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(block, output)\n\u001b[0;32m    760\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m         output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m block(output)\n\u001b[0;32m    762\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:449\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m output_patches: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m unfold(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39moutput_shift, window_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size) \\\n\u001b[0;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_windows \u001b[39melse\u001b[39;00m output_shift\n\u001b[0;32m    448\u001b[0m \u001b[39m# Perform window attention\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m output_attention: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_attention(output_patches, mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_mask)\n\u001b[0;32m    450\u001b[0m \u001b[39m# Merge patches\u001b[39;00m\n\u001b[0;32m    451\u001b[0m output_merge: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m fold(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39moutput_attention, window_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, height\u001b[39m=\u001b[39mheight,\n\u001b[0;32m    452\u001b[0m                                   width\u001b[39m=\u001b[39mwidth) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_windows \u001b[39melse\u001b[39;00m output_attention\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:302\u001b[0m, in \u001b[0;36mWindowMultiHeadAttention.forward\u001b[1;34m(self, input, mask)\u001b[0m\n\u001b[0;32m    297\u001b[0m     output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__sequential_self_attention(query\u001b[39m=\u001b[39mquery, key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39mvalue,\n\u001b[0;32m    298\u001b[0m                                                             batch_size_windows\u001b[39m=\u001b[39mbatch_size_windows,\n\u001b[0;32m    299\u001b[0m                                                             tokens\u001b[39m=\u001b[39mtokens,\n\u001b[0;32m    300\u001b[0m                                                             mask\u001b[39m=\u001b[39mmask)\n\u001b[0;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m     output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__self_attention(query\u001b[39m=\u001b[39;49mquery, key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49mvalue,\n\u001b[0;32m    303\u001b[0m                                                  batch_size_windows\u001b[39m=\u001b[39;49mbatch_size_windows, tokens\u001b[39m=\u001b[39;49mtokens,\n\u001b[0;32m    304\u001b[0m                                                  mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m    305\u001b[0m \u001b[39m# Perform linear mapping and dropout\u001b[39;00m\n\u001b[0;32m    306\u001b[0m output: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(output))\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\swin_transformer_v2\\model_parts.py:211\u001b[0m, in \u001b[0;36mWindowMultiHeadAttention.__self_attention\u001b[1;34m(self, query, key, value, batch_size_windows, tokens, mask)\u001b[0m\n\u001b[0;32m    209\u001b[0m attention_map: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m attention_map \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39m# Apply relative positional encodings\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m attention_map: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m attention_map \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_relative_positional_encodings()\n\u001b[0;32m    212\u001b[0m \u001b[39m# Apply mask if utilized\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dummy=torch.randn(4, 3, 256, 128)\n",
    "logits=swin_transformer(dummy)\n",
    "\n",
    "print(logits[0].shape)\n",
    "print(logits[1].shape)\n",
    "print(logits[2].shape)\n",
    "print(logits[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 96, 64, 64])\n",
      "torch.Size([4, 192, 32, 32])\n",
      "torch.Size([4, 384, 16, 16])\n",
      "torch.Size([4, 768, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(logits[0].shape)\n",
    "print(logits[1].shape)\n",
    "print(logits[2].shape)\n",
    "print(logits[3].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swin_transformer_v2 import SwinTransformerV2\n",
    "\n",
    "from swin_transformer_v2 import swin_transformer_v2_t, swin_transformer_v2_s, swin_transformer_v2_b, \\\n",
    "    swin_transformer_v2_l, swin_transformer_v2_h, swin_transformer_v2_g\n",
    "\n",
    "# SwinV2-T\n",
    "swin_transformer: SwinTransformerV2 = swin_transformer_v2_t(in_channels=3,\n",
    "                                                            window_size=(6, 20),\n",
    "                                                            input_resolution=(192, 640),\n",
    "                                                            sequential_self_attention=False,\n",
    "                                                            use_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 96, 48, 160])\n",
      "torch.Size([4, 192, 24, 80])\n",
      "torch.Size([4, 384, 12, 40])\n",
      "torch.Size([4, 768, 6, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dummy=torch.randn(4, 3, 192, 640)\n",
    "result=swin_transformer(dummy)\n",
    "print(result[0].shape)\n",
    "print(result[1].shape)\n",
    "print(result[2].shape)\n",
    "print(result[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768, 12, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "x=torch.randn(4, 768, 6, 20)\n",
    "x=F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 112, 112])\n",
      "torch.Size([8, 64, 56, 56])\n",
      "torch.Size([8, 128, 28, 28])\n",
      "torch.Size([8, 256, 14, 14])\n",
      "torch.Size([8, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model=timm.create_model('resnet34', features_only=True)\n",
    "dummy=torch.randn(8,3,224,224)\n",
    "\n",
    "for i in model(dummy):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SwinTransformerV2' object has no attribute 'feature_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mh:\\내 드라이브\\CV-paper\\Network\\Monodepth2\\Dataset_check.ipynb 셀 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m=\u001b[39mtimm\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m'\u001b[39;49m\u001b[39mswinv2_tiny_window8_256\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, features_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dummy\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrandn(\u001b[39m8\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/CV-paper/Network/Monodepth2/Dataset_check.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model(dummy)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\factory.py:71\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, pretrained_cfg, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m create_fn \u001b[39m=\u001b[39m model_entrypoint(model_name)\n\u001b[0;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m set_layer_config(scriptable\u001b[39m=\u001b[39mscriptable, exportable\u001b[39m=\u001b[39mexportable, no_jit\u001b[39m=\u001b[39mno_jit):\n\u001b[1;32m---> 71\u001b[0m     model \u001b[39m=\u001b[39m create_fn(pretrained\u001b[39m=\u001b[39mpretrained, pretrained_cfg\u001b[39m=\u001b[39mpretrained_cfg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_path:\n\u001b[0;32m     74\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\swin_transformer_v2.py:655\u001b[0m, in \u001b[0;36mswinv2_tiny_window8_256\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    653\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m    654\u001b[0m     window_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, embed_dim\u001b[39m=\u001b[39m\u001b[39m96\u001b[39m, depths\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m2\u001b[39m), num_heads\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m24\u001b[39m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 655\u001b[0m \u001b[39mreturn\u001b[39;00m _create_swin_transformer_v2(\u001b[39m'\u001b[39m\u001b[39mswinv2_tiny_window8_256\u001b[39m\u001b[39m'\u001b[39m, pretrained\u001b[39m=\u001b[39mpretrained, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\swin_transformer_v2.py:633\u001b[0m, in \u001b[0;36m_create_swin_transformer_v2\u001b[1;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_swin_transformer_v2\u001b[39m(variant, pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 633\u001b[0m     model \u001b[39m=\u001b[39m build_model_with_cfg(\n\u001b[0;32m    634\u001b[0m         SwinTransformerV2, variant, pretrained,\n\u001b[0;32m    635\u001b[0m         pretrained_filter_fn\u001b[39m=\u001b[39mcheckpoint_filter_fn,\n\u001b[0;32m    636\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    637\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\helpers.py:572\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[1;34m(model_cls, variant, pretrained, pretrained_cfg, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, pretrained_custom_load, kwargs_filter, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m             \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown feature class \u001b[39m\u001b[39m{\u001b[39;00mfeature_cls\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 572\u001b[0m model \u001b[39m=\u001b[39m feature_cls(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfeature_cfg)\n\u001b[0;32m    573\u001b[0m model\u001b[39m.\u001b[39mpretrained_cfg \u001b[39m=\u001b[39m pretrained_cfg_for_features(pretrained_cfg)  \u001b[39m# add back default_cfg\u001b[39;00m\n\u001b[0;32m    574\u001b[0m model\u001b[39m.\u001b[39mdefault_cfg \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpretrained_cfg  \u001b[39m# alias for backwards compat\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\features.py:227\u001b[0m, in \u001b[0;36mFeatureListNet.__init__\u001b[1;34m(self, model, out_indices, out_map, feature_concat, flatten_sequential)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    225\u001b[0m         \u001b[39mself\u001b[39m, model,\n\u001b[0;32m    226\u001b[0m         out_indices\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m), out_map\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feature_concat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, flatten_sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 227\u001b[0m     \u001b[39msuper\u001b[39;49m(FeatureListNet, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    228\u001b[0m         model, out_indices\u001b[39m=\u001b[39;49mout_indices, out_map\u001b[39m=\u001b[39;49mout_map, feature_concat\u001b[39m=\u001b[39;49mfeature_concat,\n\u001b[0;32m    229\u001b[0m         flatten_sequential\u001b[39m=\u001b[39;49mflatten_sequential)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\features.py:181\u001b[0m, in \u001b[0;36mFeatureDictNet.__init__\u001b[1;34m(self, model, out_indices, out_map, feature_concat, flatten_sequential)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    178\u001b[0m         \u001b[39mself\u001b[39m, model,\n\u001b[0;32m    179\u001b[0m         out_indices\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m), out_map\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feature_concat\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, flatten_sequential\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    180\u001b[0m     \u001b[39msuper\u001b[39m(FeatureDictNet, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_info \u001b[39m=\u001b[39m _get_feature_info(model, out_indices)\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcat \u001b[39m=\u001b[39m feature_concat\n\u001b[0;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\timm\\models\\features.py:136\u001b[0m, in \u001b[0;36m_get_feature_info\u001b[1;34m(net, out_indices)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_feature_info\u001b[39m(net, out_indices):\n\u001b[1;32m--> 136\u001b[0m     feature_info \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(net, \u001b[39m'\u001b[39;49m\u001b[39mfeature_info\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_info, FeatureInfo):\n\u001b[0;32m    138\u001b[0m         \u001b[39mreturn\u001b[39;00m feature_info\u001b[39m.\u001b[39mfrom_other(out_indices)\n",
      "File \u001b[1;32mc:\\Users\\wongyun\\miniconda3\\envs\\torchstudy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SwinTransformerV2' object has no attribute 'feature_info'"
     ]
    }
   ],
   "source": [
    "model=timm.create_model('swinv2_tiny_window8_256', pretrained=True, features_only=True)\n",
    "dummy=torch.randn(8,3,256,256)\n",
    "\n",
    "model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=timm.create_model('swin_v2_cr_tiny_ns_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=timm.create_model('vgg19', features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 256, 256])\n",
      "torch.Size([8, 128, 128, 128])\n",
      "torch.Size([8, 256, 64, 64])\n",
      "torch.Size([8, 512, 32, 32])\n",
      "torch.Size([8, 512, 16, 16])\n",
      "torch.Size([8, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "for i in model(dummy):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 768, 7, 7])\n",
      "torch.Size([8, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(model.forward_features(dummy).shape)\n",
    "print(model.forward_features(dummy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='tmp/mono_model/models/pretrained/swin_v2_cr_tiny_ns_224-ba8166c6.pth'\n",
    "pretrained_dir='tmp/mono_model/models/weights_19/pose_encoder.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_model=torch.load(train_dir)\n",
    "pretrained_model=torch.load(pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.conv1.weight', 'encoder.bn1.weight', 'encoder.bn1.bias', 'encoder.bn1.running_mean', 'encoder.bn1.running_var', 'encoder.bn1.num_batches_tracked', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.bn1.num_batches_tracked', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.bn2.num_batches_tracked', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.bn1.num_batches_tracked', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.bn2.num_batches_tracked', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.bn1.num_batches_tracked', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.bn2.num_batches_tracked', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.0.downsample.1.num_batches_tracked', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.bn1.num_batches_tracked', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.bn2.num_batches_tracked', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.bn1.num_batches_tracked', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.bn2.num_batches_tracked', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.0.downsample.1.num_batches_tracked', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.bn1.num_batches_tracked', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.bn2.num_batches_tracked', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.bn1.num_batches_tracked', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.bn2.num_batches_tracked', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.0.downsample.1.num_batches_tracked', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.bn1.num_batches_tracked', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.bn2.num_batches_tracked', 'encoder.fc.weight', 'encoder.fc.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'stages.0.blocks.0.attn.tau', 'stages.0.blocks.0.attn.qkv.weight', 'stages.0.blocks.0.attn.qkv.bias', 'stages.0.blocks.0.attn.proj.weight', 'stages.0.blocks.0.attn.proj.bias', 'stages.0.blocks.0.attn.meta_mlp.fc1.weight', 'stages.0.blocks.0.attn.meta_mlp.fc1.bias', 'stages.0.blocks.0.attn.meta_mlp.fc2.weight', 'stages.0.blocks.0.attn.meta_mlp.fc2.bias', 'stages.0.blocks.0.norm1.weight', 'stages.0.blocks.0.norm1.bias', 'stages.0.blocks.0.mlp.fc1.weight', 'stages.0.blocks.0.mlp.fc1.bias', 'stages.0.blocks.0.mlp.fc2.weight', 'stages.0.blocks.0.mlp.fc2.bias', 'stages.0.blocks.0.norm2.weight', 'stages.0.blocks.0.norm2.bias', 'stages.0.blocks.1.attn.tau', 'stages.0.blocks.1.attn.qkv.weight', 'stages.0.blocks.1.attn.qkv.bias', 'stages.0.blocks.1.attn.proj.weight', 'stages.0.blocks.1.attn.proj.bias', 'stages.0.blocks.1.attn.meta_mlp.fc1.weight', 'stages.0.blocks.1.attn.meta_mlp.fc1.bias', 'stages.0.blocks.1.attn.meta_mlp.fc2.weight', 'stages.0.blocks.1.attn.meta_mlp.fc2.bias', 'stages.0.blocks.1.norm1.weight', 'stages.0.blocks.1.norm1.bias', 'stages.0.blocks.1.mlp.fc1.weight', 'stages.0.blocks.1.mlp.fc1.bias', 'stages.0.blocks.1.mlp.fc2.weight', 'stages.0.blocks.1.mlp.fc2.bias', 'stages.0.blocks.1.norm2.weight', 'stages.0.blocks.1.norm2.bias', 'stages.0.blocks.1.norm3.weight', 'stages.0.blocks.1.norm3.bias', 'stages.1.downsample.norm.weight', 'stages.1.downsample.norm.bias', 'stages.1.downsample.reduction.weight', 'stages.1.blocks.0.attn.tau', 'stages.1.blocks.0.attn.qkv.weight', 'stages.1.blocks.0.attn.qkv.bias', 'stages.1.blocks.0.attn.proj.weight', 'stages.1.blocks.0.attn.proj.bias', 'stages.1.blocks.0.attn.meta_mlp.fc1.weight', 'stages.1.blocks.0.attn.meta_mlp.fc1.bias', 'stages.1.blocks.0.attn.meta_mlp.fc2.weight', 'stages.1.blocks.0.attn.meta_mlp.fc2.bias', 'stages.1.blocks.0.norm1.weight', 'stages.1.blocks.0.norm1.bias', 'stages.1.blocks.0.mlp.fc1.weight', 'stages.1.blocks.0.mlp.fc1.bias', 'stages.1.blocks.0.mlp.fc2.weight', 'stages.1.blocks.0.mlp.fc2.bias', 'stages.1.blocks.0.norm2.weight', 'stages.1.blocks.0.norm2.bias', 'stages.1.blocks.1.attn.tau', 'stages.1.blocks.1.attn.qkv.weight', 'stages.1.blocks.1.attn.qkv.bias', 'stages.1.blocks.1.attn.proj.weight', 'stages.1.blocks.1.attn.proj.bias', 'stages.1.blocks.1.attn.meta_mlp.fc1.weight', 'stages.1.blocks.1.attn.meta_mlp.fc1.bias', 'stages.1.blocks.1.attn.meta_mlp.fc2.weight', 'stages.1.blocks.1.attn.meta_mlp.fc2.bias', 'stages.1.blocks.1.norm1.weight', 'stages.1.blocks.1.norm1.bias', 'stages.1.blocks.1.mlp.fc1.weight', 'stages.1.blocks.1.mlp.fc1.bias', 'stages.1.blocks.1.mlp.fc2.weight', 'stages.1.blocks.1.mlp.fc2.bias', 'stages.1.blocks.1.norm2.weight', 'stages.1.blocks.1.norm2.bias', 'stages.1.blocks.1.norm3.weight', 'stages.1.blocks.1.norm3.bias', 'stages.2.downsample.norm.weight', 'stages.2.downsample.norm.bias', 'stages.2.downsample.reduction.weight', 'stages.2.blocks.0.attn.tau', 'stages.2.blocks.0.attn.qkv.weight', 'stages.2.blocks.0.attn.qkv.bias', 'stages.2.blocks.0.attn.proj.weight', 'stages.2.blocks.0.attn.proj.bias', 'stages.2.blocks.0.attn.meta_mlp.fc1.weight', 'stages.2.blocks.0.attn.meta_mlp.fc1.bias', 'stages.2.blocks.0.attn.meta_mlp.fc2.weight', 'stages.2.blocks.0.attn.meta_mlp.fc2.bias', 'stages.2.blocks.0.norm1.weight', 'stages.2.blocks.0.norm1.bias', 'stages.2.blocks.0.mlp.fc1.weight', 'stages.2.blocks.0.mlp.fc1.bias', 'stages.2.blocks.0.mlp.fc2.weight', 'stages.2.blocks.0.mlp.fc2.bias', 'stages.2.blocks.0.norm2.weight', 'stages.2.blocks.0.norm2.bias', 'stages.2.blocks.1.attn.tau', 'stages.2.blocks.1.attn.qkv.weight', 'stages.2.blocks.1.attn.qkv.bias', 'stages.2.blocks.1.attn.proj.weight', 'stages.2.blocks.1.attn.proj.bias', 'stages.2.blocks.1.attn.meta_mlp.fc1.weight', 'stages.2.blocks.1.attn.meta_mlp.fc1.bias', 'stages.2.blocks.1.attn.meta_mlp.fc2.weight', 'stages.2.blocks.1.attn.meta_mlp.fc2.bias', 'stages.2.blocks.1.norm1.weight', 'stages.2.blocks.1.norm1.bias', 'stages.2.blocks.1.mlp.fc1.weight', 'stages.2.blocks.1.mlp.fc1.bias', 'stages.2.blocks.1.mlp.fc2.weight', 'stages.2.blocks.1.mlp.fc2.bias', 'stages.2.blocks.1.norm2.weight', 'stages.2.blocks.1.norm2.bias', 'stages.2.blocks.2.attn.tau', 'stages.2.blocks.2.attn.qkv.weight', 'stages.2.blocks.2.attn.qkv.bias', 'stages.2.blocks.2.attn.proj.weight', 'stages.2.blocks.2.attn.proj.bias', 'stages.2.blocks.2.attn.meta_mlp.fc1.weight', 'stages.2.blocks.2.attn.meta_mlp.fc1.bias', 'stages.2.blocks.2.attn.meta_mlp.fc2.weight', 'stages.2.blocks.2.attn.meta_mlp.fc2.bias', 'stages.2.blocks.2.norm1.weight', 'stages.2.blocks.2.norm1.bias', 'stages.2.blocks.2.mlp.fc1.weight', 'stages.2.blocks.2.mlp.fc1.bias', 'stages.2.blocks.2.mlp.fc2.weight', 'stages.2.blocks.2.mlp.fc2.bias', 'stages.2.blocks.2.norm2.weight', 'stages.2.blocks.2.norm2.bias', 'stages.2.blocks.3.attn.tau', 'stages.2.blocks.3.attn.qkv.weight', 'stages.2.blocks.3.attn.qkv.bias', 'stages.2.blocks.3.attn.proj.weight', 'stages.2.blocks.3.attn.proj.bias', 'stages.2.blocks.3.attn.meta_mlp.fc1.weight', 'stages.2.blocks.3.attn.meta_mlp.fc1.bias', 'stages.2.blocks.3.attn.meta_mlp.fc2.weight', 'stages.2.blocks.3.attn.meta_mlp.fc2.bias', 'stages.2.blocks.3.norm1.weight', 'stages.2.blocks.3.norm1.bias', 'stages.2.blocks.3.mlp.fc1.weight', 'stages.2.blocks.3.mlp.fc1.bias', 'stages.2.blocks.3.mlp.fc2.weight', 'stages.2.blocks.3.mlp.fc2.bias', 'stages.2.blocks.3.norm2.weight', 'stages.2.blocks.3.norm2.bias', 'stages.2.blocks.4.attn.tau', 'stages.2.blocks.4.attn.qkv.weight', 'stages.2.blocks.4.attn.qkv.bias', 'stages.2.blocks.4.attn.proj.weight', 'stages.2.blocks.4.attn.proj.bias', 'stages.2.blocks.4.attn.meta_mlp.fc1.weight', 'stages.2.blocks.4.attn.meta_mlp.fc1.bias', 'stages.2.blocks.4.attn.meta_mlp.fc2.weight', 'stages.2.blocks.4.attn.meta_mlp.fc2.bias', 'stages.2.blocks.4.norm1.weight', 'stages.2.blocks.4.norm1.bias', 'stages.2.blocks.4.mlp.fc1.weight', 'stages.2.blocks.4.mlp.fc1.bias', 'stages.2.blocks.4.mlp.fc2.weight', 'stages.2.blocks.4.mlp.fc2.bias', 'stages.2.blocks.4.norm2.weight', 'stages.2.blocks.4.norm2.bias', 'stages.2.blocks.5.attn.tau', 'stages.2.blocks.5.attn.qkv.weight', 'stages.2.blocks.5.attn.qkv.bias', 'stages.2.blocks.5.attn.proj.weight', 'stages.2.blocks.5.attn.proj.bias', 'stages.2.blocks.5.attn.meta_mlp.fc1.weight', 'stages.2.blocks.5.attn.meta_mlp.fc1.bias', 'stages.2.blocks.5.attn.meta_mlp.fc2.weight', 'stages.2.blocks.5.attn.meta_mlp.fc2.bias', 'stages.2.blocks.5.norm1.weight', 'stages.2.blocks.5.norm1.bias', 'stages.2.blocks.5.mlp.fc1.weight', 'stages.2.blocks.5.mlp.fc1.bias', 'stages.2.blocks.5.mlp.fc2.weight', 'stages.2.blocks.5.mlp.fc2.bias', 'stages.2.blocks.5.norm2.weight', 'stages.2.blocks.5.norm2.bias', 'stages.2.blocks.5.norm3.weight', 'stages.2.blocks.5.norm3.bias', 'stages.3.downsample.norm.weight', 'stages.3.downsample.norm.bias', 'stages.3.downsample.reduction.weight', 'stages.3.blocks.0.attn.tau', 'stages.3.blocks.0.attn.qkv.weight', 'stages.3.blocks.0.attn.qkv.bias', 'stages.3.blocks.0.attn.proj.weight', 'stages.3.blocks.0.attn.proj.bias', 'stages.3.blocks.0.attn.meta_mlp.fc1.weight', 'stages.3.blocks.0.attn.meta_mlp.fc1.bias', 'stages.3.blocks.0.attn.meta_mlp.fc2.weight', 'stages.3.blocks.0.attn.meta_mlp.fc2.bias', 'stages.3.blocks.0.norm1.weight', 'stages.3.blocks.0.norm1.bias', 'stages.3.blocks.0.mlp.fc1.weight', 'stages.3.blocks.0.mlp.fc1.bias', 'stages.3.blocks.0.mlp.fc2.weight', 'stages.3.blocks.0.mlp.fc2.bias', 'stages.3.blocks.0.norm2.weight', 'stages.3.blocks.0.norm2.bias', 'stages.3.blocks.1.attn.tau', 'stages.3.blocks.1.attn.qkv.weight', 'stages.3.blocks.1.attn.qkv.bias', 'stages.3.blocks.1.attn.proj.weight', 'stages.3.blocks.1.attn.proj.bias', 'stages.3.blocks.1.attn.meta_mlp.fc1.weight', 'stages.3.blocks.1.attn.meta_mlp.fc1.bias', 'stages.3.blocks.1.attn.meta_mlp.fc2.weight', 'stages.3.blocks.1.attn.meta_mlp.fc2.bias', 'stages.3.blocks.1.norm1.weight', 'stages.3.blocks.1.norm1.bias', 'stages.3.blocks.1.mlp.fc1.weight', 'stages.3.blocks.1.mlp.fc1.bias', 'stages.3.blocks.1.mlp.fc2.weight', 'stages.3.blocks.1.mlp.fc2.bias', 'stages.3.blocks.1.norm2.weight', 'stages.3.blocks.1.norm2.bias', 'stages.3.blocks.1.norm3.weight', 'stages.3.blocks.1.norm3.bias', 'head.weight', 'head.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "N, C, H, W = 4, 8, 8, 768\n",
    "input = torch.randn(N, C, H, W)\n",
    "norm=nn.LayerNorm(768)\n",
    "\n",
    "result=norm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool=torch.randn(4, 768, 1)\n",
    "pool_new=torch.flatten(pool, 1)\n",
    "pool_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "x=torch.randn(4, 32, 128)\n",
    "\n",
    "layers = nn.Sequential(\n",
    "    nn.Linear(96*2, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def x():\n",
    "    a=3\n",
    "    print(a)\n",
    "    return a\n",
    "\n",
    "k=[]\n",
    "k.append(x())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torchstudy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2a29e84b0c43c20c166437993d835aa1a00044783bb297f1abc4254f32cfca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
